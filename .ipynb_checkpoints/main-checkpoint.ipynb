{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Monkey Species Image Dataset Analysis\n",
    "\n",
    "Module: CS985 Machine Learning For Data Analytics\n",
    "\n",
    "Student: Rokas Labeikis\n",
    "\n",
    "Student ID: 201349799\n",
    "\n",
    "## Objective and problem\n",
    "\n",
    "The core problem of the assignment is to classify any image (with monkeys in it) from a list into 1 of 10 classes.\n",
    "\n",
    "The objective is to create an sufficient classifier which performs well. However, more importantly for the author is to learn and explore machine learning framework Tensorflow. Due to the later reason, more than 1 approach of image classification will be researched and the performance of the models will not be considered a priority.\n",
    "\n",
    "Explored techniques include:\n",
    "- Linear image classification;\n",
    "- Transfer learning;\n",
    "- Convolutional Neural Networks.\n",
    "\n",
    "## Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TensorFlow version: ', '1.7.0')\n",
      "('Python version: ', '2.7.13')\n"
     ]
    }
   ],
   "source": [
    "# Importing neccesary modules\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import platform\n",
    "from __future__ import division\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# Versions used\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('Python version: ', platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen dataset was taken from www.kaggle.com and it is called \"10 Monkey Species\". The original dataset consists of 2 directories (\"training\" and \"validation\") and a text file containing all the labels and titles. Each directory  contains 10 subdirectories, labeled from \"n0\" to \"n9\". Finally, each subdirectory contains a series of images (images are 400x300 px or larger and JPEG format) of a certain class.\n",
    "\n",
    "Below the contents of the \"monkey_labels.txt\" file are presented. The table encompasses the overall structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Latin Name</th>\n",
       "      <th>Common Name</th>\n",
       "      <th>Train Images</th>\n",
       "      <th>Validation Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n0</td>\n",
       "      <td>alouatta_palliata\\t</td>\n",
       "      <td>mantled_howler</td>\n",
       "      <td>131</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n1</td>\n",
       "      <td>erythrocebus_patas\\t</td>\n",
       "      <td>patas_monkey</td>\n",
       "      <td>139</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n2</td>\n",
       "      <td>cacajao_calvus\\t</td>\n",
       "      <td>bald_uakari</td>\n",
       "      <td>137</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n3</td>\n",
       "      <td>macaca_fuscata\\t</td>\n",
       "      <td>japanese_macaque</td>\n",
       "      <td>152</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n4</td>\n",
       "      <td>cebuella_pygmea\\t</td>\n",
       "      <td>pygmy_marmoset</td>\n",
       "      <td>131</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n5</td>\n",
       "      <td>cebus_capucinus\\t</td>\n",
       "      <td>white_headed_capuchin</td>\n",
       "      <td>141</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n6</td>\n",
       "      <td>mico_argentatus\\t</td>\n",
       "      <td>silvery_marmoset</td>\n",
       "      <td>132</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n7</td>\n",
       "      <td>saimiri_sciureus\\t</td>\n",
       "      <td>common_squirrel_monkey</td>\n",
       "      <td>142</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n8</td>\n",
       "      <td>aotus_nigriceps\\t</td>\n",
       "      <td>black_headed_night_monkey</td>\n",
       "      <td>133</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n9</td>\n",
       "      <td>trachypithecus_johnii</td>\n",
       "      <td>nilgiri_langur</td>\n",
       "      <td>132</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                Latin Name                      Common Name  \\\n",
       "0  n0         alouatta_palliata\\t    mantled_howler                   \n",
       "1  n1        erythrocebus_patas\\t    patas_monkey                     \n",
       "2  n2        cacajao_calvus\\t        bald_uakari                      \n",
       "3  n3        macaca_fuscata\\t        japanese_macaque                 \n",
       "4  n4       cebuella_pygmea\\t        pygmy_marmoset                   \n",
       "5  n5       cebus_capucinus\\t        white_headed_capuchin            \n",
       "6  n6       mico_argentatus\\t        silvery_marmoset                 \n",
       "7  n7      saimiri_sciureus\\t        common_squirrel_monkey           \n",
       "8  n8       aotus_nigriceps\\t        black_headed_night_monkey        \n",
       "9  n9       trachypithecus_johnii    nilgiri_langur                   \n",
       "\n",
       "   Train Images  Validation Images  \n",
       "0           131                 26  \n",
       "1           139                 28  \n",
       "2           137                 27  \n",
       "3           152                 30  \n",
       "4           131                 26  \n",
       "5           141                 28  \n",
       "6           132                 26  \n",
       "7           142                 28  \n",
       "8           133                 27  \n",
       "9           132                 26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = pd.read_csv(\"input/monkey_labels.txt\", names=['Label','Latin Name', 'Common Name','Train Images', 'Validation Images'], skiprows=1)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining input directory paths\n",
    "train_dir = 'input/training'\n",
    "test_dir = 'input/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below \"read_data\" function is defined. This function takes an directory as an input and produces a data frame after its done. The produced data frame consist of numerical image representation, its label, path to the real image, integer encoded label and one hot encoded label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    images = []\n",
    "    labels = []\n",
    "    paths = []\n",
    "    i = 0\n",
    "    for dir_path in glob.glob(filename + \"/*\"):\n",
    "        label = dir_path.split(\"/\")[-1]\n",
    "        for image_path in glob.glob(os.path.join(dir_path, \"*.jpg\")):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            # Adjust stored image size if necessary\n",
    "            image = cv2.resize(image, (1, 1))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            labels.append(label)\n",
    "            images.append(image)\n",
    "            paths.append(image_path)\n",
    "            \n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(labels)\n",
    "    # one hot binary encode \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    i_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(i_encoded)\n",
    "    \n",
    "    d = {'images':images,'labels':labels, 'path': paths, 'int':integer_encoded, 'hot': onehot_encoded.tolist()}\n",
    "\n",
    "    df = pd.DataFrame(d)\n",
    "    return df\n",
    "\n",
    "train = read_data(train_dir)\n",
    "test = read_data(test_dir)\n",
    "# Write data to csv files for future use \n",
    "train.to_csv('./input/train.csv', index=False) \n",
    "test.to_csv('./input/test.csv',  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hot</th>\n",
       "      <th>images</th>\n",
       "      <th>int</th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[215, 209, 194]]]</td>\n",
       "      <td>3</td>\n",
       "      <td>n3</td>\n",
       "      <td>input/training/n3/n3049.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[90, 73, 65]]]</td>\n",
       "      <td>3</td>\n",
       "      <td>n3</td>\n",
       "      <td>input/training/n3/n3133.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[170, 167, 157]]]</td>\n",
       "      <td>3</td>\n",
       "      <td>n3</td>\n",
       "      <td>input/training/n3/n3170.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[142, 132, 133]]]</td>\n",
       "      <td>3</td>\n",
       "      <td>n3</td>\n",
       "      <td>input/training/n3/n3064.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[228, 235, 245]]]</td>\n",
       "      <td>3</td>\n",
       "      <td>n3</td>\n",
       "      <td>input/training/n3/n3135.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 hot               images  \\\n",
       "0  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[215, 209, 194]]]   \n",
       "1  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     [[[90, 73, 65]]]   \n",
       "2  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[170, 167, 157]]]   \n",
       "3  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[142, 132, 133]]]   \n",
       "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[228, 235, 245]]]   \n",
       "\n",
       "   int labels                         path  \n",
       "0    3     n3  input/training/n3/n3049.jpg  \n",
       "1    3     n3  input/training/n3/n3133.jpg  \n",
       "2    3     n3  input/training/n3/n3170.jpg  \n",
       "3    3     n3  input/training/n3/n3064.jpg  \n",
       "4    3     n3  input/training/n3/n3135.jpg  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #generated dataframe for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some checks to make sure everything went smoothly with the input reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hot       0\n",
       "images    0\n",
       "int       0\n",
       "labels    0\n",
       "path      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum() # checking for missing values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hot       0\n",
       "images    0\n",
       "int       0\n",
       "labels    0\n",
       "path      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'hot', u'images', u'int', u'labels', u'path'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "There are 1096 images dedicated for model training and 272 for testing. In total there are 1368 number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 5)\n",
      "(272, 5)\n",
      "('Total entries: ', 1368)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(\"Total entries: \", len(train)+len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "9 images along with their labels are printed below. This is done in order to see the type of images we are dealing with and if the labels are matching correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcZQkgBjKEkA/BfflkkQUF4uEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6f9836050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(images, labels):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images.iloc[i])  \n",
    "\n",
    "        ax.set_xlabel(labels.iloc[i])\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# Get 10 random images from the test-set.\n",
    "sample = test.sample(n=9)\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(sample[\"images\"], sample[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model\n",
    "\n",
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n1', 0.99499714, 'Correct!')\n",
      "('n7', 0.9958555, 'Correct!')\n",
      "('n6', 0.9874325, 'Correct!')\n",
      "('n3', 0.978956, 'Correct!')\n",
      "('n5', 0.99827945, 'Correct!')\n",
      "('n6', 0.96657157, 'Correct!')\n",
      "('n7', 0.9926466, 'Correct!')\n",
      "('n5', 0.9963971, 'Correct!')\n",
      "('n2', 0.9078938, 'Correct!')\n",
      "('n1', 0.99499714, 'Correct!')\n",
      "('n0', 0.99846387, 'Correct!')\n",
      "('n1', 0.92250466, 'Correct!')\n",
      "('n8', 0.99902844, 'Correct!')\n",
      "('n4', 0.95408446, 'Correct!')\n",
      "('n7', 0.983564, 'Correct!')\n",
      "('n0', 0.99984777, 'Correct!')\n",
      "('n2', 0.99017286, 'Correct!')\n",
      "('n5', 0.9656844, 'Correct!')\n",
      "('n9', 0.9427001, 'Correct!')\n",
      "('n3', 0.91738963, 'Correct!')\n",
      "('n6', 0.9933854, 'Correct!')\n",
      "('n8', 0.9983078, 'Correct!')\n",
      "('n3', 0.9998202, 'Correct!')\n",
      "('n9', 0.9985574, 'Correct!')\n",
      "('n1', 0.99181753, 'Correct!')\n",
      "('n1', 0.9928041, 'Correct!')\n",
      "('n9', 0.9948605, 'Correct!')\n",
      "('n0', 0.9980363, 'Correct!')\n",
      "('n6', 0.9931766, 'Correct!')\n",
      "('n8', 0.95426494, 'Correct!')\n",
      "('n5', 0.98561585, 'Correct!')\n",
      "('n8', 0.99942183, 'Correct!')\n",
      "('n9', 0.9328075, 'Correct!')\n",
      "('n3', 0.99068004, 'Correct!')\n",
      "('n3', 0.99876297, 'Correct!')\n",
      "('n7', 0.98138386, 'Correct!')\n",
      "('n6', 0.99725795, 'Correct!')\n",
      "('n8', 0.95493245, 'Correct!')\n",
      "('n9', 0.9982712, 'Correct!')\n",
      "('n8', 0.99836046, 'Correct!')\n",
      "('n2', 0.9798202, 'Correct!')\n",
      "('n4', 0.9978346, 'Correct!')\n",
      "('n2', 0.90155524, 'Correct!')\n",
      "('n2', 0.99307775, 'Correct!')\n",
      "('n7', 0.9893579, 'Correct!')\n",
      "('n6', 0.99316025, 'Correct!')\n",
      "('n2', 0.991376, 'Correct!')\n",
      "('n1', 0.99983263, 'Correct!')\n",
      "('n2', 0.9919585, 'Correct!')\n",
      "('n1', 0.9990539, 'Correct!')\n",
      "('n2', 0.9868662, 'Correct!')\n",
      "('n0', 0.99784434, 'Correct!')\n",
      "('n1', 0.9414028, 'Correct!')\n",
      "('n0', 0.99883157, 'Correct!')\n",
      "('n4', 0.98901844, 'Correct!')\n",
      "('n9', 0.99978715, 'Correct!')\n",
      "('n3', 0.9613882, 'Correct!')\n",
      "('n6', 0.9862213, 'Correct!')\n",
      "('n4', 0.9826324, 'Correct!')\n",
      "('n3', 0.9959925, 'Correct!')\n",
      "('n8', 0.69780123, 'Correct!')\n",
      "('n3', 0.99618983, 'Correct!')\n",
      "('n5', 0.9948907, 'Correct!')\n",
      "('n5', 0.9656844, 'Correct!')\n",
      "('n9', 0.9745361, 'Correct!')\n",
      "('n5', 0.9991825, 'Correct!')\n",
      "('n2', 0.9728319, 'Correct!')\n",
      "('n0', 0.99976987, 'Correct!')\n",
      "('n7', 0.9975823, 'Correct!')\n",
      "('n3', 0.9936706, 'Correct!')\n",
      "('n8', 0.99835336, 'Correct!')\n",
      "('n3', 0.9714076, 'Correct!')\n",
      "('n1', 0.9955812, 'Correct!')\n",
      "('n8', 0.99877673, 'Correct!')\n",
      "('n8', 0.96788156, 'Correct!')\n",
      "('n9', 0.99851114, 'Correct!')\n",
      "('n7', 0.9982743, 'Correct!')\n",
      "('n4', 0.987281, 'Correct!')\n",
      "('n5', 0.91539156, 'Correct!')\n",
      "('n0', 0.99854314, 'Correct!')\n",
      "('n9', 0.9893824, 'Correct!')\n",
      "('n8', 0.99535733, 'Correct!')\n",
      "('n9', 0.9898683, 'Correct!')\n",
      "('n3', 0.71840626, 'Correct!')\n",
      "('n7', 0.9897397, 'Correct!')\n",
      "('n7', 0.98530245, 'Correct!')\n",
      "('n0', 0.9836023, 'Correct!')\n",
      "('n3', 0.99478275, 'Correct!')\n",
      "('n4', 0.9976826, 'Correct!')\n",
      "('n9', 0.97409123, 'Correct!')\n",
      "('n7', 0.98754543, 'Correct!')\n",
      "('n5', 0.9974246, 'Correct!')\n",
      "('n5', 0.9973953, 'Correct!')\n",
      "('n4', 0.98490036, 'Correct!')\n",
      "('n0', 0.966275, 'Correct!')\n",
      "('n7', 0.9967836, 'Correct!')\n",
      "('n6', 0.98291934, 'Correct!')\n",
      "('n1', 0.9954367, 'Correct!')\n",
      "('n8', 0.9966987, 'Correct!')\n",
      "('n9', 0.9658403, 'Correct!')\n",
      "('n3', 0.9991844, 'Correct!')\n",
      "('n2', 0.9514423, 'Correct!')\n",
      "('n1', 0.9989085, 'Correct!')\n",
      "('n9', 0.99556124, 'Correct!')\n",
      "('n2', 0.9048977, 'Correct!')\n",
      "('n3', 0.99067616, 'Correct!')\n",
      "('n3', 0.9747661, 'Correct!')\n",
      "('n8', 0.9979867, 'Correct!')\n",
      "('n6', 0.84626085, 'Correct!')\n",
      "('n9', 0.9712577, 'Correct!')\n",
      "('n2', 0.95616525, 'Correct!')\n",
      "('n6', 0.9898173, 'Correct!')\n",
      "('n1', 0.9996847, 'Correct!')\n",
      "('n9', 0.9960997, 'Correct!')\n",
      "('n1', 0.999902, 'Correct!')\n",
      "('n2', 0.9789438, 'Correct!')\n",
      "('n4', 0.9970003, 'Correct!')\n",
      "('n3', 0.99857223, 'Correct!')\n",
      "('n6', 0.9017547, 'Correct!')\n",
      "('n8', 0.99911076, 'Correct!')\n",
      "('n1', 0.99972767, 'Correct!')\n",
      "('n3', 0.97580534, 'Correct!')\n",
      "('n2', 0.76783943, 'Correct!')\n",
      "('n5', 0.99494606, 'Correct!')\n",
      "('n1', 0.98489916, 'Correct!')\n",
      "('n1', 0.99988747, 'Correct!')\n",
      "('n1', 0.999705, 'Correct!')\n",
      "('n0', 0.9994771, 'Correct!')\n",
      "('n5', 0.98120433, 'Correct!')\n",
      "('n6', 0.9991861, 'Correct!')\n",
      "('n7', 0.9980416, 'Correct!')\n",
      "('n8', 0.9833994, 'Correct!')\n",
      "('n1', 0.9993968, 'Correct!')\n",
      "('n0', 0.99981743, 'Correct!')\n",
      "('n2', 0.9942426, 'Correct!')\n",
      "('n6', 0.9691245, 'Correct!')\n",
      "('n9', 0.99970514, 'Correct!')\n",
      "('n7', 0.99681145, 'Correct!')\n",
      "('n4', 0.9716442, 'Correct!')\n",
      "('n5', 0.9983993, 'Correct!')\n",
      "('n3', 0.97424316, 'Correct!')\n",
      "('n3', 0.97569877, 'Correct!')\n",
      "('n2', 0.9806651, 'Correct!')\n",
      "('n4', 0.98731905, 'Correct!')\n",
      "('n6', 0.99867576, 'Correct!')\n",
      "('n6', 0.9912889, 'Correct!')\n",
      "('n7', 0.99598795, 'Correct!')\n",
      "('n2', 0.9980089, 'Correct!')\n",
      "('n4', 0.9996519, 'Correct!')\n",
      "('n5', 0.99284714, 'Correct!')\n",
      "('n0', 0.99992, 'Correct!')\n",
      "('n4', 0.9919471, 'Correct!')\n",
      "('n2', 0.8582761, 'Correct!')\n",
      "('n2', 0.68529564, 'Correct!')\n",
      "('n9', 0.997042, 'Correct!')\n",
      "('n5', 0.987832, 'Correct!')\n",
      "('n3', 0.99884254, 'Correct!')\n",
      "('n2', 0.9957172, 'Correct!')\n",
      "('n3', 0.91383415, 'Correct!')\n",
      "('n5', 0.6668329, 'Correct!')\n",
      "('n5', 0.9983399, 'Correct!')\n",
      "('n4', 0.9834451, 'Correct!')\n",
      "('n0', 0.9967391, 'Correct!')\n",
      "('n8', 0.99878806, 'Correct!')\n",
      "('n7', 0.95161825, 'Correct!')\n",
      "('n6', 0.90691686, 'Correct!')\n",
      "('n9', 0.99098104, 'Correct!')\n",
      "('n6', 0.98327935, 'Correct!')\n",
      "('n2', 0.8850829, 'Correct!')\n",
      "('n4', 0.9431643, 'Correct!')\n",
      "('n3', 0.9990796, 'Correct!')\n",
      "('n1', 0.9329815, 'Correct!')\n",
      "('n4', 0.9967854, 'Correct!')\n",
      "('n9', 0.9981692, 'Correct!')\n",
      "('n7', 0.9920512, 'Correct!')\n",
      "('n1', 0.9992067, 'Correct!')\n",
      "('n6', 0.96152085, 'Correct!')\n",
      "('n1', 0.99975234, 'Correct!')\n",
      "('n5', 0.9954485, 'Correct!')\n",
      "('n5', 0.9634659, 'Correct!')\n",
      "('n7', 0.9911308, 'Correct!')\n",
      "('n8', 0.9946957, 'Correct!')\n",
      "('n6', 0.99274135, 'Correct!')\n",
      "('n8', 0.8222556, 'Correct!')\n",
      "('n5', 0.99486685, 'Correct!')\n",
      "('n7', 0.9919503, 'Correct!')\n",
      "('n4', 0.96554536, 'Correct!')\n",
      "('n6', 0.9882989, 'Correct!')\n",
      "('n3', 0.98770416, 'Correct!')\n",
      "('n1', 0.9999145, 'Correct!')\n",
      "('n0', 0.99961275, 'Correct!')\n",
      "('n4', 0.9988049, 'Correct!')\n",
      "('n7', 0.9979942, 'Correct!')\n",
      "('n0', 0.99803025, 'Correct!')\n",
      "('n3', 0.95417374, 'Correct!')\n",
      "('n7', 0.98804396, 'Correct!')\n",
      "('n7', 0.99859744, 'Correct!')\n",
      "('n2', 0.98542005, 'Correct!')\n",
      "('n7', 0.99615854, 'Correct!')\n",
      "('n7', 0.99222046, 'Correct!')\n",
      "('n9', 0.99585754, 'Correct!')\n",
      "('n4', 0.98059344, 'Correct!')\n",
      "('n8', 0.99592704, 'Correct!')\n",
      "('n9', 0.9902473, 'Correct!')\n",
      "('n0', 0.9993869, 'Correct!')\n",
      "('n1', 0.9995178, 'Correct!')\n",
      "('n1', 0.9954367, 'Correct!')\n",
      "('n5', 0.9400519, 'Correct!')\n",
      "('n8', 0.9997919, 'Correct!')\n",
      "('n5', 0.998329, 'Correct!')\n",
      "('n8', 0.9946719, 'Correct!')\n",
      "('n4', 0.99267477, 'Correct!')\n",
      "('n3', 0.9919876, 'Correct!')\n",
      "('n0', 0.99242777, 'Correct!')\n",
      "('n5', 0.70508075, 'Correct!')\n",
      "('n6', 0.96281177, 'Correct!')\n",
      "('n4', 0.9990957, 'Correct!')\n",
      "('n8', 0.9620383, 'Correct!')\n",
      "('n5', 0.97442436, 'Correct!')\n",
      "('n6', 0.99347115, 'Correct!')\n",
      "('n3', 0.99267536, 'Correct!')\n",
      "('n9', 0.57756686, 'Correct!')\n",
      "('n8', 0.9985807, 'Correct!')\n",
      "('n0', 0.9976878, 'Correct!')\n",
      "('n4', 0.9992454, 'Correct!')\n",
      "('n0', 0.99981254, 'Correct!')\n",
      "('n0', 0.9807421, 'Correct!')\n",
      "('n1', 0.9995696, 'Correct!')\n",
      "('n9', 0.88600916, 'Correct!')\n",
      "('n7', 0.985761, 'Correct!')\n",
      "('n0', 0.9991386, 'Correct!')\n",
      "('n0', 0.99932444, 'Correct!')\n",
      "('n3', 0.99886084, 'Correct!')\n",
      "('n0', 0.9924936, 'Correct!')\n",
      "('n8', 0.97433823, 'Correct!')\n",
      "('n6', 0.9924476, 'Correct!')\n",
      "('n1', 0.9939932, 'Correct!')\n",
      "('n2', 0.97932976, 'Correct!')\n",
      "('n7', 0.9978974, 'Correct!')\n",
      "('n8', 0.9969896, 'Correct!')\n",
      "('n5', 0.99649185, 'Correct!')\n",
      "('n0', 0.9977858, 'Correct!')\n",
      "('n8', 0.9977912, 'Correct!')\n",
      "('n5', 0.9885151, 'Correct!')\n",
      "('n7', 0.99525094, 'Correct!')\n",
      "('n4', 0.9817213, 'Correct!')\n",
      "('n4', 0.9837847, 'Correct!')\n",
      "('n6', 0.7406921, 'Correct!')\n",
      "('n3', 0.995823, 'Correct!')\n",
      "('n4', 0.99729544, 'Correct!')\n",
      "('n6', 0.9842584, 'Correct!')\n",
      "('n0', 0.99669063, 'Correct!')\n",
      "('n3', 0.99042106, 'Correct!')\n",
      "('n9', 0.98839885, 'Correct!')\n",
      "('n0', 0.9993869, 'Correct!')\n",
      "('n2', 0.9917589, 'Correct!')\n",
      "('n6', 0.606592, 'False! -Correct was: ', 'n2')\n",
      "('n4', 0.9797519, 'Correct!')\n",
      "('n3', 0.9987381, 'Correct!')\n",
      "('n1', 0.9996942, 'Correct!')\n",
      "('n7', 0.997106, 'Correct!')\n",
      "('n8', 0.9993987, 'Correct!')\n",
      "('n1', 0.9997837, 'Correct!')\n",
      "('n9', 0.72389966, 'Correct!')\n",
      "('n5', 0.9922909, 'Correct!')\n",
      "('n7', 0.95312774, 'Correct!')\n",
      "('n2', 0.99875927, 'Correct!')\n",
      "('n6', 0.98199826, 'Correct!')\n",
      "('n9', 0.9988549, 'Correct!')\n",
      "('n4', 0.98835236, 'Correct!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n5', 0.9853543, 'Correct!')\n",
      "('n2', 0.9703625, 'Correct!')\n",
      "('Accuracy: ', 1)\n"
     ]
    }
   ],
   "source": [
    "def load_labels(label_file):\n",
    "    label = []\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "    for l in proto_as_ascii_lines:\n",
    "        label.append(l.rstrip())\n",
    "    return label\n",
    "\n",
    "def read_tensor_from_image_file(file_name,\n",
    "                                input_height=299,\n",
    "                                input_width=299,\n",
    "                                input_mean=0,\n",
    "                                input_std=255):\n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    file_reader = tf.read_file(file_name, input_name)\n",
    "   \n",
    "    image_reader = tf.image.decode_jpeg(file_reader, channels=3, name=\"jpeg_reader\")\n",
    "    \n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0)\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "    \n",
    "    return result\n",
    "\n",
    "model_file = 'retrained_model/output_graph.pb'\n",
    "label_file = 'retrained_model/output_labels.txt'\n",
    "input_layer = 'Placeholder'\n",
    "output_layer = 'final_result'\n",
    "\n",
    "#load graph\n",
    "graph = tf.Graph()\n",
    "graph_def = tf.GraphDef()\n",
    "with open(model_file, \"rb\") as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "with graph.as_default():\n",
    "    tf.import_graph_def(graph_def)\n",
    "\n",
    "input_name = \"import/\" + input_layer\n",
    "output_name = \"import/\" + output_layer\n",
    "input_operation = graph.get_operation_by_name(input_name)\n",
    "output_operation = graph.get_operation_by_name(output_name)\n",
    "\n",
    "labels = load_labels(label_file)\n",
    "\n",
    "#shuffle test dataframe\n",
    "test_Tlearning = test.sample(frac=1)\n",
    "correct = 0    \n",
    "for index, row in test_Tlearning.iterrows():\n",
    "    t = read_tensor_from_image_file(row[\"path\"])\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        results = sess.run(output_operation.outputs[0], {\n",
    "            input_operation.outputs[0]: t\n",
    "        })\n",
    "    results = np.squeeze(results)\n",
    "    \n",
    "    top_k = results.argsort()[-1:][::-1]\n",
    "    for i in top_k:\n",
    "        if (labels[i] == row[\"labels\"]):\n",
    "            print(labels[i], results[i], \"Correct!\")\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            print(labels[i], results[i], \"False! -Correct was: \", row[\"labels\"], \"Path:\", row[\"path\"])\n",
    "            \n",
    "\n",
    "acc = correct / len(test_paths)\n",
    "print(\"Accuracy: \", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labelled_input_fn(df, batch_size):\n",
    "    def input_fn():\n",
    "        image_list, label_list = df[\"path\"].tolist(), df[\"int\"].tolist()\n",
    "\n",
    "        def read_images_from_disk(input_queue):\n",
    "            label = input_queue[1]\n",
    "            file_contents = tf.read_file(input_queue[0])\n",
    "            example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "            example=tf.image.resize_images(example, [150, 150])\n",
    "            example=tf.reshape(example, [150, 150, 3]) \n",
    "            return example, label\n",
    "\n",
    "        images = tf.convert_to_tensor(image_list, dtype=tf.string)\n",
    "        labels = tf.convert_to_tensor(label_list, dtype=tf.int32)\n",
    "\n",
    "        # Makes an input queue\n",
    "        input_queue = tf.train.slice_input_producer([images, labels], shuffle=True)\n",
    "\n",
    "        image, label = read_images_from_disk(input_queue)\n",
    "        image_batch, label_batch = tf.train.batch([image, label], batch_size=batch_size)\n",
    "        return {'images':  image_batch}, label_batch\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc6c4a5d50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './linear_model', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column('images', shape=[150,150,3])]\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=10,\n",
    "    model_dir='./linear_model'\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "TRAIN_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./linear_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 92.10341, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./linear_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3939168.5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7efc198f9390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=generate_labelled_input_fn(train, BATCH_SIZE),\n",
    "    steps=TRAIN_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-11-21:52:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./linear_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-11-21:53:49\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.33825, average_loss = 90258.18, global_step = 100, loss = 3610327.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.33825,\n",
       " 'average_loss': 90258.18,\n",
       " 'global_step': 100,\n",
       " 'loss': 3610327.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(\n",
    "    input_fn=generate_labelled_input_fn(test, BATCH_SIZE),\n",
    "    steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
